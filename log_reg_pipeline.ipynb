{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "dataframe = pd.read_csv(\"SBAnational.csv\", low_memory=False)\n",
    "#dataframe.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataframe.drop('LoanNr_ChkDgt', axis=1)\n",
    "X = X.drop('Name', axis=1)\n",
    "X = X.drop('City', axis=1)\n",
    "# State included\n",
    "X = X.drop('Zip', axis=1)\n",
    "X = X.drop('Bank', axis=1)\n",
    "X = X.drop('BankState', axis=1)\n",
    "# NAICS included (2 first chars )\n",
    "X = X.drop('ApprovalDate', axis=1)\n",
    "# ApprovalFY included \n",
    "# Term (nombre de mois - à mettre en rapport avec le GrAppv ?) included\n",
    "# NoEmp, NewExist , CreateJob, RetainedJob, FranchiseCode, UrbanRural, RevLineCr include \n",
    "# LowDoc (include 'y' only) included\n",
    "X = X.drop('ChgOffDate', axis=1) # (explicit end on simulation?) : included \n",
    "X = X.drop('DisbursementDate', axis=1)  \n",
    "X = X.drop('DisbursementGross', axis=1)  \n",
    "X = X.drop('BalanceGross', axis=1)  # 14 valeurs seulement différentes de zéro  ?\n",
    "# MIS_Status : INCLUDED !!! because it is Y\n",
    "X = X.drop('ChgOffPrinGr', axis=1)\n",
    "# GrAppv : included\n",
    "# SBA_Appv : included\n",
    "\n",
    "\n",
    "\n",
    "#X.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert data types\n",
    "    (and remove lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in X.columns.values :\n",
    "    print(f\"{column_name} : {X[column_name].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"RevLineCr\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conversion_functions as conv\n",
    "import math\n",
    "\n",
    "print (f\"Before : {X.shape[0]} lines\")\n",
    "\n",
    "explanable_X = pd.DataFrame(X)\n",
    "\n",
    "explanable_X[\"State\"] = explanable_X[\"State\"].apply(lambda x : conv.get_state_code(x))\n",
    "#explanable_X = explanable_X[explanable_X[\"State\"]!=0]\n",
    "\n",
    "explanable_X[\"NAICS\"] = explanable_X[\"NAICS\"].apply(lambda x : conv.get_NAICS_data(x))\n",
    "#explanable_X = explanable_X[explanable_X[\"NAICS\"]!=0]\n",
    "\n",
    "explanable_X[\"ApprovalFY\"] = explanable_X[\"ApprovalFY\"].apply(lambda x : conv.get_ApprovalFY_data(x)) \n",
    "mean_dataframe = explanable_X[ explanable_X[\"ApprovalFY\"] !=50]\n",
    "mean_value = mean_dataframe[\"ApprovalFY\"].mean()\n",
    "explanable_X.loc[explanable_X['ApprovalFY'] == 50, 'ApprovalFY'] = math.ceil(mean_value)\n",
    "\n",
    "explanable_X[\"NewExist\"] = explanable_X[\"NewExist\"].apply(lambda x : conv.get_NewExist_data(x)) \n",
    "explanable_X[\"FranchiseCode\"] = explanable_X[\"FranchiseCode\"].apply(lambda x : conv.get_FranchiseCode_data(x)) \n",
    "\n",
    "explanable_X = explanable_X.drop(\"UrbanRural\", axis=1)\n",
    "\n",
    "explanable_X[\"RevLineCr\"] = explanable_X[\"RevLineCr\"].apply(lambda x : conv.get_RevLineCr_data(x)) \n",
    "explanable_X[\"LowDoc\"] = explanable_X[\"LowDoc\"].apply(lambda x : conv.get_LowDoc_data(x)) \n",
    "\n",
    "explanable_X[\"GrAppv\"] = explanable_X[\"GrAppv\"].apply(lambda x : conv.get_GrAppv_value(x)) \n",
    "mean_dataframe = explanable_X[ explanable_X[\"GrAppv\"] !=0]\n",
    "mean_value = mean_dataframe[\"GrAppv\"].mean()\n",
    "explanable_X.loc[explanable_X['GrAppv'] == 0, 'ApprovalFY'] = math.ceil(mean_value)\n",
    "\n",
    "explanable_X = explanable_X[ explanable_X[\"GrAppv\"] !=0] \n",
    "\n",
    "explanable_X = explanable_X.drop(\"SBA_Appv\", axis = 1)\n",
    "# explanable_X[\"SBA_Appv\"] = explanable_X[\"SBA_Appv\"].apply(lambda x : conv.get_SBA_Appv_value(x)) \n",
    "# explanable_X = explanable_X[ explanable_X[\"SBA_Appv\"] !=0] \n",
    "\n",
    "explanable_X = explanable_X[(explanable_X[\"MIS_Status\"]==\"P I F\") |\n",
    "                              (explanable_X[\"MIS_Status\"]==\"CHGOFF\")]\n",
    "\n",
    "explanable_X[\"MIS_Status\"] = explanable_X[\"MIS_Status\"].apply(lambda x : 1.0 if x == \"CHGOFF\" else 0.0)\n",
    "\n",
    "print (f\"After : {explanable_X.shape[0]} lines\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in explanable_X.columns.values :\n",
    "    print(f\"{column_name} : {explanable_X[column_name].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OrdinalEncoder, LabelBinarizer, OneHotEncoder, Binarizer, FunctionTransformer, PolynomialFeatures,MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_pipeline(  \n",
    "    make_column_transformer(\n",
    "        # LoanNr_ChkDgt, Name , City : excluded\n",
    "        (OneHotEncoder(), [\"State\"]), \n",
    "        # Zip , Bank , BankState : excluded\n",
    "        (OneHotEncoder(), [\"NAICS\"]), #included (2 first chars )\n",
    "        # ApprovalDate : excluded\n",
    "        (StandardScaler(), [\"ApprovalFY\"]),\n",
    "        (StandardScaler(), [\"Term\"]), \n",
    "        (StandardScaler(), [\"NoEmp\"]),\n",
    "        (StandardScaler(), [\"NewExist\"]),\n",
    "        (StandardScaler(), [\"CreateJob\"]),\n",
    "        (StandardScaler(), [\"RetainedJob\"]),\n",
    "        (Binarizer(), [\"FranchiseCode\"]),\n",
    "        #Binarizer(\"UrbanRural\", threshold=1.5),\n",
    "        (OneHotEncoder(), [\"RevLineCr\"]),\n",
    "        (Binarizer(), [\"LowDoc\"]),\n",
    "        # SimpleImputer(\"ChgOffDate\"), # explicit end on simulation\n",
    "        # DisbursementDate, DisbursementGross , BalanceGross ( 14 valeurs seulement différentes de zéro  ?)\n",
    "        # MIS_Status : Excluded because it is Y !!!\n",
    "        # ChgOffPrinGr : excluded\n",
    "        (StandardScaler(), [\"GrAppv\"]), \n",
    "        #(StandardScaler(),[\"SBA_Appv\"]),  \n",
    "        remainder='passthrough'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_model = LogisticRegression(max_iter=200, solver='liblinear', random_state=0)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor), \n",
    "    #('vectorizer', TfidfVectorizer()), \n",
    "    ('model', log_reg_model)])\n",
    "\n",
    "pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = explanable_X[\"MIS_Status\"]\n",
    "print(y)\n",
    "explanable_X = explanable_X.drop(\"MIS_Status\", axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanable_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(explanable_X, y, shuffle=True, train_size=0.8, random_state=42, stratify=y)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# print(y.value_counts())\n",
    "# print(f\"X.X    {y.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pipeline.score(X_test, y_test)\n",
    "print(f'Précision du modèle : {score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Évaluer la performance du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(f\"Exactitude du modèle (Accuracy) : {accuracy * 100:.2f}%\")\n",
    "print(\"Matrice de confusion :\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des features\n",
    "log_reg:LogisticRegression = pipeline.named_steps['model']\n",
    "\n",
    "# Récupérer les coefficients de la régression logistique\n",
    "\n",
    "columnTransformer:ColumnTransformer  = pipeline.named_steps['preprocessor'][0]\n",
    "\n",
    "# Afficher les colonnes transformées\n",
    "\n",
    "feature_names = []\n",
    "remainder_num=0\n",
    "for transformer_name, transformer, columns in columnTransformer.transformers_:\n",
    "    if str(transformer_name).startswith(\"onehotencoder\") :\n",
    "        num = 0\n",
    "        onehot:OneHotEncoder = transformer\n",
    "        column_name = columns[0]\n",
    "        for n in transformer.get_feature_names_out():\n",
    "            feature_names.append(f\"{columns[0]}_{num}\")\n",
    "            num+=1\n",
    "    else :\n",
    "        if len(columns) == 1 :\n",
    "            name = columns[0]\n",
    "            if name == \"Term\" :\n",
    "                name = \"Echéance\"\n",
    "            \n",
    "            feature_names.append(name) \n",
    "        else:\n",
    "            num = 0\n",
    "            for column in columns : \n",
    "                feature_names.append(f\"other_{column}_{num}\")\n",
    "                num+=1\n",
    "            \n",
    "\n",
    "coefficients = log_reg.coef_[0]\n",
    "\n",
    "print(f\"feature names : {len(feature_names)}\")\n",
    "print(f\"Coefficients : {len(coefficients)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Créer un DataFrame avec les features et leurs coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "# Trier par coefficient (en valeur absolue) pour voir les features les plus importantes\n",
    "feature_importance['AbsCoefficient'] = feature_importance['Coefficient'].abs()\n",
    "feature_importance_sorted = feature_importance.sort_values(by='AbsCoefficient', ascending=False)\n",
    "\n",
    "# Afficher les résultats\n",
    "#print(feature_importance_sorted[['Feature', 'Coefficient']])\n",
    "\n",
    "# afficher les résultats sans le state et le naics\n",
    "\n",
    "selection = feature_importance_sorted[ ~(feature_importance_sorted[\"Feature\"].str.startswith(\"State\") | feature_importance_sorted[\"Feature\"].str.startswith(\"NAICS\") )]\n",
    "print(selection[['Feature', 'Coefficient']])\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# need ipywidget and matplotlib\n",
    "transformed_X_train = pipeline.named_steps['preprocessor'].transform(X_train)\n",
    "\n",
    "# Initialiser SHAP explainer\n",
    "# Créer un explainer avec LinearExplainer en passant le modèle et les feature_names\n",
    "explainer = shap.LinearExplainer(pipeline.named_steps['model'], transformed_X_train, feature_names=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_X_test = pipeline.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "# Calcul des SHAP values sur le jeu de test\n",
    "shap_values = explainer.shap_values(transformed_X_test)\n",
    "\n",
    "# Affichage du summary plot\n",
    "shap.summary_plot(shap_values, transformed_X_test, feature_names=feature_names, max_display=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
